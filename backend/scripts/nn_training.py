# -*- coding: utf-8 -*-
"""nn_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bxvw6BXvGT15kg6H_NAfulJ8EN6hDV8s

### Install required packages for training model
"""

# !pip install gensim --upgrade
# !pip install tensorflow-gpu --upgrade
# !pip install keras --upgrade
# !pip install pandas --upgrade

"""### Import packages"""

# Commented out IPython magic to ensure Python compatibility.
# DataFrame
import pandas as pd

# Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

# Word2vec
from gensim.models import Word2Vec

# Keras
import keras
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras.models import Model, Sequential
from keras.layers import Input, Activation, Dense, Dropout, Embedding, Conv1D, Concatenate, Bidirectional, LSTM, GlobalMaxPool1D
from keras.callbacks import ReduceLROnPlateau, EarlyStopping

# Utils
import os
import re
import pickle
import numpy as np
import time
import itertools

# Plots
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline

"""# Data Preparation

### Import Data

The dataset file location should be

"../content/dataset/training.1600000.processed.noemoticon.csv"
"""

# Read raw dataset
dataset_columns = ["sentiment", "ids", "date", "flag", "user", "text"]
dataset_encoding = "ISO-8859-1"
dataset = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',
                      encoding=dataset_encoding , names=dataset_columns)
dataset.head(5)

# Removing the unnecessary columns.
dataset = dataset[['sentiment','text']]

# Replace value 4 with 1 (positive)
dataset['sentiment'] = dataset['sentiment'].replace(4,1)

ax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data', legend=False)
ax = ax.set_xticklabels(['Negative', 'Positive'], rotation=0)

"""### Pre-Process dataset"""

# Load Enlgish contraction dictionary
contractions = pd.read_csv('/content/contractions.csv', index_col='Contraction')
contractions.index = contractions.index.str.lower()
contractions.Meaning = contractions.Meaning.str.lower()
contractions_dict = contractions.to_dict()['Meaning']

# Define text cleaning pattern
urlPattern        = r"((http://)[^ ]*|(https://)[^ ]*|(www\.)[^ ]*)"
userPattern       = '@[^\s]+'
hashtagPattern    = '#[^\s]+'
alphaPattern      = "[^a-z0-9<>]"
sequencePattern   = r"(.)\1\1+"
seqReplacePattern = r"\1\1"

# Define emojis cleaning pattern
smileemoji        = r"[8:=;]['`\-]?[)d]+"
sademoji          = r"[8:=;]['`\-]?\(+"
neutralemoji      = r"[8:=;]['`\-]?[\/|l*]"
lolemoji          = r"[8:=;]['`\-]?p+"

def preprocess(text):
    
    # 1, Convert to lower case
    text = text.lower()

    # 2, Replace all URls with '<url>'
    text = re.sub(urlPattern, '<url>', text)
    
    # 3, Replace all @USERNAME to '<user>'.
    text = re.sub(userPattern, '<user>', text)
    
    # 4, Replace 3 or more consecutive letters by 2 letter.
    text = re.sub(sequencePattern, seqReplacePattern, text)

    # 5, Replace all emojis.
    text = re.sub(r'<3', '<heart>', text)
    text = re.sub(smileemoji, '<smile>', text)
    text = re.sub(sademoji, '<sadface>', text)
    text = re.sub(neutralemoji, '<neutralface>', text)
    text = re.sub(lolemoji, '<lolface>', text)

    # 6, Remove Contractions
    for contraction, replacement in contractions_dict.items():
        text = text.replace(contraction, replacement)

    # 7, Removing Non-Alphabets and replace them with a space
    text = re.sub(alphaPattern, ' ', text)

    # 8, Adding space on either side of '/' to seperate words.
    text = re.sub(r'/', ' / ', text)
    
    return text

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Clean up the text and store it in a new field
# dataset['processed_text'] = dataset.text.apply(preprocess)

# Show a sample processed result
sample_idx = 10
print("Original  Text: ", dataset.iloc[sample_idx][1])
print("Processed Text: ", dataset.iloc[sample_idx][2])

"""### Split training and testing set"""

X_data, y_data = np.array(dataset['processed_text']), np.array(dataset['sentiment'])

X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,
                                                    test_size = 0.05, random_state = 0)
print("TRAIN size:", len(y_train))
print("TEST size:", len(y_test))

"""### Word embedding"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# w2v_model_path = '/content/w2v-twitter-100'
# 
# # Load the word2vec model if previously trained
# if os.path.exists(tokenizer_path):
#     w2v_model = Word2Vec.load(w2v_model_path)
# 
# # Train a new word2vec model
# else:
#     # Parameters
#     w2v_size = 100
#     w2v_window = 7
#     w2v_min_count = 10
# 
#     # Training set for w2v model
#     w2v_training_data = list(map(lambda x: x.split(), X_train))
# 
#     # Defining w2v model and training it.
#     w2v_model = Word2Vec(w2v_training_data,
#                          vector_size = w2v_size,
#                          workers = w2v_window,
#                          min_count = w2v_min_count)

# Demonstrate result
print("Vocabulary Length:", len(w2v_model.wv.key_to_index))
print("Similar words to 'love'")
w2v_model.wv.most_similar("love")

"""### Tokenize text"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# tokenizer_path = '../content/model/Tokenizer.pickle'
# 
# # Load the tokenizer if previously trained
# if os.path.exists(tokenizer_path):
#     tokenizer = pickle.load(open(tokenizer_path, 'rb'))
#     
# # Train a tokenizer
# else:
#     vocab_size = 100000
#     tokenizer = Tokenizer(filters="", lower=False, oov_token="<oov>")
#     tokenizer.fit_on_texts(X_data)
#     tokenizer.num_words = vocab_size
# 
# # vocab_size = len(tokenizer.word_index) + 1
# print("Total words", tokenizer.num_words)

"""### Padding text"""

input_length = 60

X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)
X_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)

print("X_train.shape:", X_train.shape)
print("X_test.shape :", X_test.shape)

"""# Model Building And Training

### Build Model
"""

# Embedding layer
def EmbeddingLayer():
    
    # Create embedding matrix
    embedding_matrix = np.zeros((vocab_size, w2v_size))
    # print("Embedding Matrix Shape:", embedding_matrix.shape)

    for word, i in tokenizer.word_index.items():
        if word in w2v_model.wv:
            embedding_matrix[i] = w2v_model.wv[word]

    # Return embedding layer
    return Embedding(input_dim = vocab_size,
                     output_dim = w2v_size,
                     weights = [embedding_matrix],
                     input_length = input_length,
                     trainable = False)

# Bi-LSTM + CNN
def Bi_LSTM():
    model = Sequential([
        EmbeddingLayer(),
        Bidirectional(LSTM(60, dropout=0.3, return_sequences=True)),
        Bidirectional(LSTM(60, dropout=0.3, return_sequences=True)),
        Conv1D(60, kernel_size=5, activation='relu'),
        GlobalMaxPool1D(), 
        Dense(1, activation='sigmoid'),
    ])
    
    return model

# C-LSTM Model
def C_LSTM():
    input_layer = Input(shape = (input_length))
    embedding_layer = EmbeddingLayer()
    x = embedding_layer(input_layer)
    x = Dropout(0.2)(x)

    conv2 = Conv1D(16, kernel_size=2, activation='relu')(x)
    conv3 = Conv1D(16, kernel_size=3, activation='relu')(x)
    conv5 = Conv1D(16, kernel_size=5, activation='relu')(x)
    conv7 = Conv1D(16, kernel_size=7, activation='relu')(x)
    conv_len = conv7.shape[1]
    x = Concatenate(axis=-1)([conv2[:,:conv_len,:], 
                              conv3[:,:conv_len,:], 
                              conv5[:,:conv_len,:], 
                              conv7[:,:conv_len,:]])

    x = LSTM(128, dropout=0.2, activation='relu', return_sequences=True)(x)
    x = LSTM(128, dropout=0.2, activation='relu')(x)

    x = Dense(1, activation='sigmoid')(x)

    model = Model(inputs=input_layer, outputs=x)
    
    return model

# LSTM Model
def Pure_LSTM():
    model = Sequential()
    model.add(EmbeddingLayer())
    model.add(Dropout(0.5))
    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dense(1, activation='sigmoid'))
    
    return model

import keras
from keras_preprocessing.sequence import pad_sequences
import tensorflow as tf
tf.get_logger().setLevel('WARNING')


def padding(sequence, maxlen=60):
    # Padding sequences
    return pad_sequences(sequence, maxlen=maxlen)


def c_lstm():
    model_path = '/content/C-LSTM.h5'
    model = keras.models.load_model(model_path)
    return model

"""### Loading or Training Model"""

load_model = True
# Load a model if already trained
#if load_model:
#   model = keras.models.load_model(model_path)


# Train a new model
#else:
model = C_LSTM()
    # model = Bi_LSTM()
    # model = Pure_LSTM()
    
model.compile(loss='binary_crossentropy',
                  optimizer="adam",
                  metrics=['accuracy'])
    
    
model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# if not load_model:
# 
#     # Training parameters
#     epochs = 6
#     batch_size = 512
# 
#     # Define callbacks
#     callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\
#                  EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]
# 
#     # Start training
#     history = model.fit(X_train, y_train,
#                         batch_size = batch_size,
#                         epochs = epochs,
#                         validation_split = 0.1,
#                         verbose = 1,
#                         callbacks = callbacks)

"""### Evaluate"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# score = model.evaluate(X_test, y_test, batch_size = 512)
# print()
# print("ACCURACY:",score[1])
# print("LOSS:",score[0])

# Training parameters
epochs = 6
batch_size = 512

    # Define callbacks
callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\
                 EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]

    # Start training
history = model.fit(X_train, y_train,
                        batch_size = batch_size,
                        epochs = epochs,
                        validation_split = 0.1,
                        verbose = 1,
                        callbacks = callbacks)
score = model.evaluate(X_test, y_test, batch_size = 512)
print()
print("ACCURACY:",score[1])
print("LOSS:",score[0])
    
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
 
epochs = range(len(acc))
 
plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
 
plt.figure()
 
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
 
plt.show()

# !unzip /content/Backend-20221027T174853Z-001.zip

"""### Predict samples"""

def predict(text):
    # Pre-process text
    processed_text = preprocess(text)
    # Tokenize text
    input_matrix = pad_sequences(tokenizer.texts_to_sequences([processed_text]), maxlen=input_length)
    # Predict
    score = model.predict([input_matrix])[0]
    
    return float(score)

predict("I love the music!!")

predict("I hate the rain :(")

predict("i don't know what i'm doing")

"""# Model Analysis

### Confusion Matrix
"""

def plot_confusion_matrix(y_test, y_pred):
    # Compute and plot the Confusion matrix
    print(y_test)
    print(y_pred)
    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    plt.figure(figsize=(12,12))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.colorbar()
    
    classes  = ['Negative', 'Positive']
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)
    plt.yticks(tick_marks, classes, fontsize=22)

    fmt = '.2f'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 fontsize = 20,
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.title('Confusion Matrix', fontsize=30)
    plt.ylabel('True label', fontsize=25)
    plt.xlabel('Predicted label', fontsize=25)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Predicting on the Test dataset
# y_pred = model.predict(X_test, batch_size = batch_size)
# 
# # Converting prediction to reflect the sentiment predicted.
# y_pred = np.where(y_pred>=0.5, 1, 0)

# Printing out the Evaluation metrics. 
plot_confusion_matrix(y_test, y_pred)

"""### Classification Report"""

print(classification_report(y_test, y_pred))

"""### Save model"""

# Saving Word2Vec-Model
w2v_model.save(w2v_model_path)

# Saving the tokenizer
with open(tokenizer_path, 'wb') as file:
    pickle.dump(tokenizer, file)

# Saving the TF-Model.
model.save(model_path)

import pickle
from preprocess import Preprocess
from nn_models import padding, c_lstm


class Backend():
    def __init__(self):
        tokenizer_path = '../content/model/Tokenizer.pickle'

        # Text preprocessing and tokenizing
        self.preprocess = Preprocess()
        self.tokenizer = pickle.load(open(tokenizer_path, 'rb'))

        # Models
        self.c_lstm = c_lstm()


    def analyze_sentiment(self, text, selected_model="C-LSTM"):
        # Pre-process text
        processed_text = self.preprocess.preprocess(text)
        # Tokenize text
        tokenized_sequence = self.tokenizer.texts_to_sequences([processed_text])

        # C-LSTM model
        if selected_model == "C-LSTM":
            model = c_lstm()
            input_matrix = padding(tokenized_sequence)
            result = float( model([input_matrix])[0] )

        # Return score (0 - 1)
        return result


if __name__ == "__main__":
    backend = Backend()
    s1 = backend.analyze_sentiment("The game was wonderfullll!", "C-LSTM")
    print("The score for sentence 1 is", s1)
    s2 = backend.analyze_sentiment("I really don't like the music today :(", "C-LSTM")
    print("The score for sentence 2 is", s2)